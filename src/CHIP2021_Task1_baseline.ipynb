{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import jieba\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('/home/shencj/workspace/code/nlp/Frame/ark-nlp/')\n",
    "\n",
    "from ark_nlp.nn import Ernie\n",
    "from ark_nlp.dataset import TMDataset\n",
    "from ark_nlp.factory.task import TMTask, SequenceClassificationTask\n",
    "from ark_nlp.factory.optimizer import get_default_bert_optimizer\n",
    "from ark_nlp.processor.tokenizer.transfomer import PairTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    设置随机种子\n",
    "    :param seed:\n",
    "    \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目录地址\n",
    "\n",
    "train_data_path = '/home/shencj/workspace/data/medical/CHIP2021/Task1/train.jsonl'\n",
    "test_data_path = '/home/shencj/workspace/data/medical/CHIP2021/Task1/testa.txt'\n",
    "submit_data_path = 'submit.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、数据读入与处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 数据读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import json\n",
    "import codecs\n",
    "\n",
    "# from utils import get_entity_bios\n",
    "from ark_nlp.dataset import BaseDataset\n",
    "\n",
    "\n",
    "def get_task_data(data_path):\n",
    "    with codecs.open(data_path, mode='r', encoding='utf8') as f:\n",
    "        reader = f.readlines(f)    \n",
    "        \n",
    "    data_list = []\n",
    "\n",
    "    for dialogue_ in reader:\n",
    "        dialogue_ = json.loads(dialogue_)\n",
    "        for content_idx_, contents_ in enumerate(dialogue_['dialog_info']):\n",
    "\n",
    "            terms_ = contents_['ner']\n",
    "\n",
    "            if len(terms_) != 0:\n",
    "                idx_ = 0\n",
    "                for _, term_ in enumerate(terms_):\n",
    "                    \n",
    "                    entity_ = dict()\n",
    "\n",
    "                    entity_['dialog_id'] = dialogue_['dialog_id']\n",
    "                    \n",
    "                    entity_['text_a'] = dialogue_['dialog_info'][content_idx_]['text']\n",
    "\n",
    "                    if content_idx_ + 1 < len(dialogue_['dialog_info']):\n",
    "                        entity_['text_a'] = entity_['text_a'] + dialogue_['dialog_info'][content_idx_+1]['text']\n",
    "                    if content_idx_ - 1 >= 0:\n",
    "                        entity_['text_a'] = dialogue_['dialog_info'][content_idx_-1]['text'] + entity_['text_a']\n",
    "                        \n",
    "                    entity_['text_b'] = term_['mention']\n",
    "                    entity_['start_idx'] = term_['range'][0]\n",
    "                    entity_['end_idx'] = term_['range'][1] - 1\n",
    "                    \n",
    "                    try:\n",
    "                        entity_['label_b'] = term_['mention']\n",
    "                    except:\n",
    "                        print(contents_)\n",
    "                        print(term_)\n",
    "                    entity_['label'] = term_['attr']\n",
    "                    idx_ += 1\n",
    "                    \n",
    "                    if entity_['label'] == '':\n",
    "                        continue\n",
    "                    \n",
    "                    if len(entity_) == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    data_list.append(entity_)\n",
    "            \n",
    "    data_df = pd.DataFrame(data_list)\n",
    "    \n",
    "    data_df = data_df.loc[:,['text_a', 'text_b', 'start_idx', 'end_idx', 'label_b', 'label', 'dialog_id']]\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = get_task_data(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118976, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_dev = train_test_split(list(set(data_df['dialog_id'].tolist())))\n",
    "\n",
    "train_data_df = data_df[data_df['dialog_id'].apply(lambda x: x in X_train)]\n",
    "dev_data_df = data_df[data_df['dialog_id'].apply(lambda x: x in X_dev)]\n",
    "\n",
    "train_data_df.reset_index(drop=True, inplace=True)\n",
    "dev_data_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_train_dataset = TMDataset(train_data_df)\n",
    "tm_dev_dataset = TMDataset(dev_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 词典创建和生成分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以先创建词典，再加载入分词器\n",
    "# 也可以使用分词器自动加载\n",
    "# bert_vocab = transformers.AutoTokenizer.from_pretrained('nghuyong/ernie-1.0')\n",
    "# tokenizer = TransfomerTokenizer(bert_vocab, max_seq_len=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PairTokenizer(vocab='nghuyong/ernie-1.0', max_seq_len=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. ID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_train_dataset.convert_to_ids(tokenizer)\n",
    "tm_dev_dataset.convert_to_ids(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 二、模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 模型参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "bert_config = BertConfig.from_pretrained('nghuyong/ernie-1.0', \n",
    "                                         num_labels=len(tm_train_dataset.cat2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 模型创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nghuyong/ernie-1.0 were not used when initializing Ernie: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing Ernie from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Ernie from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Ernie were not initialized from the model checkpoint at nghuyong/ernie-1.0 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dl_module = Ernie.from_pretrained('nghuyong/ernie-1.0',\n",
    "                                  config=bert_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 三、任务构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 任务参数和必要部件设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置运行次数\n",
    "num_epoches = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_default_bert_optimizer(dl_module) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 任务创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequenceClassificationTask(dl_module, optimizer, 'ce', cuda_device=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 100/2797 [00:46<21:02,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99/2797],train loss is:0.922122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 200/2797 [01:33<20:25,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199/2797],train loss is:0.826494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 300/2797 [02:21<19:43,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299/2797],train loss is:0.768934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 400/2797 [03:08<18:58,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[399/2797],train loss is:0.728550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 500/2797 [03:56<18:14,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[499/2797],train loss is:0.698540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 600/2797 [04:44<17:26,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[599/2797],train loss is:0.679917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 700/2797 [05:31<16:41,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[699/2797],train loss is:0.662679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 800/2797 [06:19<16:02,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[799/2797],train loss is:0.651655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 900/2797 [07:07<15:23,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[899/2797],train loss is:0.638450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 1000/2797 [07:56<14:37,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[999/2797],train loss is:0.630506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 1100/2797 [08:44<13:31,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1099/2797],train loss is:0.622484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1200/2797 [09:32<12:48,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1199/2797],train loss is:0.613460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 1300/2797 [10:20<12:18,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1299/2797],train loss is:0.605594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1400/2797 [11:09<11:14,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1399/2797],train loss is:0.600252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 1500/2797 [11:57<10:22,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1499/2797],train loss is:0.593298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 1600/2797 [12:45<09:35,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1599/2797],train loss is:0.588580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 1700/2797 [13:34<08:46,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1699/2797],train loss is:0.584810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 1800/2797 [14:23<08:10,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1799/2797],train loss is:0.581599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 1900/2797 [15:12<07:09,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1899/2797],train loss is:0.578146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 2000/2797 [16:01<06:32,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1999/2797],train loss is:0.575369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 2100/2797 [16:50<05:36,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2099/2797],train loss is:0.571160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 2200/2797 [17:38<04:46,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2199/2797],train loss is:0.567543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 2300/2797 [18:26<03:58,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2299/2797],train loss is:0.564077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 2400/2797 [19:15<03:23,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2399/2797],train loss is:0.561753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 2500/2797 [20:05<02:27,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2499/2797],train loss is:0.559353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 2600/2797 [20:53<01:34,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2599/2797],train loss is:0.556748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 2700/2797 [21:41<00:46,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2699/2797],train loss is:0.554015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2797/2797 [22:28<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[0],train loss is:0.551978 \n",
      "\n",
      "classification_report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         不标注       0.65      0.73      0.69      5731\n",
      "          其他       0.47      0.29      0.36      1737\n",
      "          阳性       0.88      0.85      0.87     18605\n",
      "          阴性       0.62      0.74      0.68      3430\n",
      "\n",
      "    accuracy                           0.78     29503\n",
      "   macro avg       0.66      0.65      0.65     29503\n",
      "weighted avg       0.78      0.78      0.78     29503\n",
      "\n",
      "confusion_matrix_: \n",
      " [[ 4169    96  1237   229]\n",
      " [  237   504   420   576]\n",
      " [ 1766   279 15824   736]\n",
      " [  213   199   468  2550]]\n",
      "test loss is:0.531644,test acc is:0.781175,f1_score is:0.647538\n"
     ]
    }
   ],
   "source": [
    "model.fit(tm_train_dataset, \n",
    "          tm_dev_dataset,\n",
    "          lr=2e-5,\n",
    "          epochs=1, \n",
    "          batch_size=batch_size\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(tm_train_dataset, \n",
    "#           tm_dev_dataset,\n",
    "#           lr=2e-5,\n",
    "#           epochs=1, \n",
    "#           batch_size=batch_size\n",
    "#          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 四、模型验证与保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ark_nlp.factory.predictor import TMPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_predictor_instance = TMPredictor(model.module, tokenizer, tm_train_dataset.cat2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('不标注', 0.5857797265052795),\n",
       " ('阳性', 0.40820521116256714),\n",
       " ('其他', 0.0032178503461182117),\n",
       " ('阴性', 0.0027971782255917788)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm_predictor_instance.predict_one_sample(['医生:38.5摄氏度以上需要药物降温;医生:打得什么药物？;患者:没看;医生:不是所有生病发烧就非要输液打针;患者:打屁股的;医生:要弄清楚原因才能用药;', '38.5'], \n",
    "                                         return_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 测试结果输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [06:38<00:00,  5.02it/s]\n"
     ]
    }
   ],
   "source": [
    "submit_result = []\n",
    "\n",
    "with codecs.open(test_data_path, mode='r', encoding='utf8') as f:\n",
    "    reader = f.readlines(f)    \n",
    "\n",
    "data_list = []\n",
    "\n",
    "for dialogue_ in tqdm(reader):\n",
    "    dialogue_ = json.loads(dialogue_)\n",
    "    for content_idx_, contents_ in enumerate(dialogue_['dialog_info']):\n",
    "\n",
    "        terms_ = contents_['ner']\n",
    "\n",
    "        if len(terms_) != 0:\n",
    "            idx_ = 0\n",
    "            for _ner_idx, term_ in enumerate(terms_):\n",
    "\n",
    "                entity_ = dict()\n",
    "\n",
    "                entity_['dialog_id'] = dialogue_['dialog_id']\n",
    "\n",
    "                entity_['text_a'] = dialogue_['dialog_info'][content_idx_]['text']\n",
    "\n",
    "                if content_idx_ + 1 < len(dialogue_['dialog_info']):\n",
    "                    entity_['text_a'] = entity_['text_a'] + dialogue_['dialog_info'][content_idx_+1]['text']\n",
    "                if content_idx_ - 1 >= 0:\n",
    "                    entity_['text_a'] = dialogue_['dialog_info'][content_idx_-1]['text'] + entity_['text_a']\n",
    "\n",
    "                entity_['text_b'] = term_['mention']\n",
    "                entity_['start_idx'] = term_['range'][0]\n",
    "                entity_['end_idx'] = term_['range'][1] - 1\n",
    "\n",
    "                try:\n",
    "                    entity_['label_b'] = term_['mention']\n",
    "                except:\n",
    "                    print(contents_)\n",
    "                    print(term_)\n",
    "                entity_['label'] = term_['attr']\n",
    "                idx_ += 1\n",
    "\n",
    "                dialogue_['dialog_info'][content_idx_]['ner'][_ner_idx]['attr'] = tm_predictor_instance.predict_one_sample([entity_['text_a'], entity_['text_b']])[0]\n",
    "    submit_result.append(dialogue_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(submit_data_path, 'w') as output_data:\n",
    "    for json_content in submit_result:\n",
    "        output_data.write(json.dumps(json_content, ensure_ascii=False) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [transformers]",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
